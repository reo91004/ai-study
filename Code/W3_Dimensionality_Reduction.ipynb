{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_dim_red",
			"metadata": {},
			"source": [
				"# 차원 축소 (Dimensionality Reduction)\n",
				"\n",
				"**학습 목표:**\n",
				"- 고차원 데이터(수십~수천 개의 특성)가 가지는 문제점인 **'차원의 저주'**를 이해합니다.\n",
				"- 데이터의 분산을 최대한 보존하며 차원을 축소하는 선형 기법인 **주성분 분석(PCA)**의 원리를 배우고, **누적 설명 분산 비율**을 통해 최적의 주성분 개수를 결정하는 방법을 학습합니다.\n",
				"- 고차원 공간에서의 데이터 포인트 간 유사성을 저차원 공간에 시각화하는 비선형 기법인 **t-SNE**를 적용하고 PCA와 결과를 비교합니다.\n",
				"- MNIST 손글씨 숫자 데이터셋을 2차원으로 축소하여 시각화함으로써, 차원 축소의 효과를 직접 확인합니다."
			]
		},
		{
			"cell_type": "code",
			"id": "imports_dim_red",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"import numpy as np\n",
				"import pandas as pd\n",
				"import matplotlib.pyplot as plt\n",
				"import seaborn as sns\n",
				"\n",
				"from sklearn.datasets import fetch_openml\n",
				"from sklearn.preprocessing import StandardScaler\n",
				"from sklearn.decomposition import PCA\n",
				"from sklearn.manifold import TSNE"
			]
		},
		{
			"cell_type": "markdown",
			"id": "data_prep_dim_red",
			"metadata": {},
			"source": [
				"### (1) 데이터 준비: MNIST 손글씨 숫자\n",
				"MNIST 데이터는 28x28 픽셀의 손글씨 숫자 이미지로, 각 이미지는 784개의 특성(픽셀 값)으로 구성된 고차원 데이터입니다. 이 데이터를 저차원으로 축소하여 시각화해 보겠습니다."
			]
		},
		{
			"cell_type": "code",
			"id": "load_data_dim_red",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# fetch_openml을 통해 MNIST 데이터 로드 (시간이 다소 걸릴 수 있습니다)\n",
				"mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='liac-arff')\n",
				"\n",
				"# 계산 시간을 줄이기 위해 데이터의 일부만 사용 (예: 10000개)\n",
				"np.random.seed(42)\n",
				"random_indices = np.random.choice(mnist.data.shape[0], 10000, replace=False)\n",
				"X = mnist.data[random_indices]\n",
				"y = mnist.target[random_indices].astype(int)\n",
				"\n",
				"print(f\"Data shape: {X.shape}\")\n",
				"\n",
				"# 데이터 스케일링\n",
				"scaler = StandardScaler()\n",
				"X_scaled = scaler.fit_transform(X)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "pca_section_dim_red",
			"metadata": {},
			"source": [
				"### (2) 주성분 분석 (PCA)\n",
				"PCA는 여러 특성 간에 존재하는 상관관계를 이용하여, 이 특성들의 선형 조합으로 표현되는 새로운 축(주성분)을 찾습니다. 첫 번째 주성분은 원본 데이터의 분산을 가장 많이 설명하는 축이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 다음으로 분산을 많이 설명하는 축입니다.\n",
				"\n",
				"#### 누적 설명 분산 비율\n",
				"전체 주성분 중 몇 개를 선택해야 원본 데이터의 정보를 충분히 보존할 수 있을지 결정하는 데 사용됩니다. 보통 95%~99%의 분산을 설명하는 지점까지의 주성분을 선택합니다."
			]
		},
		{
			"cell_type": "code",
			"id": "apply_pca_dim_red",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"pca = PCA()\n",
				"pca.fit(X_scaled)\n",
				"cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
				"\n",
				"d = np.argmax(cumsum >= 0.95) + 1 # 95%의 분산을 설명하는 주성분 개수\n",
				"\n",
				"plt.figure(figsize=(8, 5))\n",
				"plt.plot(cumsum, linewidth=3)\n",
				"plt.axis([0, 400, 0, 1])\n",
				"plt.xlabel(\"Dimensions\")\n",
				"plt.ylabel(\"Explained Variance\")\n",
				"plt.plot([d, d], [0, 0.95], \"k:\")\n",
				"plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
				"plt.title(f'95% explained variance at {d} dimensions')\n",
				"plt.grid(True)\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "code",
			"id": "pca_visualize_dim_red",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# 2차원으로 축소하여 시각화\n",
				"pca_2d = PCA(n_components=2)\n",
				"X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
				"\n",
				"df_pca = pd.DataFrame(X_pca_2d, columns=['PC1', 'PC2'])\n",
				"df_pca['label'] = y\n",
				"\n",
				"plt.figure(figsize=(12, 8))\n",
				"sns.scatterplot(x='PC1', y='PC2', hue='label', data=df_pca, palette=sns.color_palette(\"hsv\", 10), s=50, alpha=0.7)\n",
				"plt.title('PCA of MNIST dataset (2 Components)')\n",
				"plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "tsne_section_dim_red",
			"metadata": {},
			"source": [
				"### (3) t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
				"t-SNE는 고차원에서의 데이터 포인트 간 지역적 유사성을 저차원에서도 최대한 보존하는 데 초점을 맞춘 비선형 시각화 기법입니다. PCA가 데이터의 전역적인 구조를 보존하려는 것과 달리, t-SNE는 비슷한 데이터들을 가까이 모으는 데 더 효과적이어서 시각화 목적으로 널리 사용됩니다."
			]
		},
		{
			"cell_type": "code",
			"id": "apply_tsne_dim_red",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# t-SNE는 계산 비용이 높으므로, PCA로 먼저 50차원 정도로 축소한 후 적용하는 것이 일반적입니다.\n",
				"pca_50 = PCA(n_components=50)\n",
				"X_pca_50 = pca_50.fit_transform(X_scaled)\n",
				"\n",
				"tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
				"X_tsne = tsne.fit_transform(X_pca_50)\n",
				"\n",
				"df_tsne = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])\n",
				"df_tsne['label'] = y\n",
				"\n",
				"plt.figure(figsize=(12, 8))\n",
				"sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='label', data=df_tsne, palette=sns.color_palette(\"hsv\", 10), s=50, alpha=0.7)\n",
				"plt.title('t-SNE of MNIST dataset')\n",
				"plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
				"plt.show()\n",
				"print(\"PCA 결과보다 t-SNE 결과에서 각 숫자 클래스가 더 명확하게 군집을 이루는 것을 볼 수 있습니다.\")"
			]
		}
	],
	"metadata": {},
	"nbformat": 4,
	"nbformat_minor": 5
}
