{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_q_learning",
			"metadata": {},
			"source": [
				"# Q-러닝(Q-Learning)으로 FrozenLake 문제 풀기\n",
				"\n",
				"**학습 목표:**\n",
				"- 에이전트가 환경과 상호작용하며 보상을 통해 학습하는 **강화학습**의 기본 개념을 이해합니다.\n",
				"- 대표적인 가치 기반(Value-based) 알고리즘인 **Q-러닝**의 동작 원리를 학습합니다.\n",
				"- 상태(State)와 행동(Action)의 가치를 저장하는 **Q-테이블**을 만들고, **벨만 방정식**을 통해 이를 업데이트하는 과정을 구현합니다.\n",
				"- 탐험(Exploration)과 활용(Exploitation)의 균형을 맞추는 **Epsilon-Greedy** 전략을 적용합니다.\n",
				"- **OpenAI Gym**의 **FrozenLake** 환경에서 미끄러운 얼음판을 건너는 에이전트를 성공적으로 훈련시킵니다."
			]
		},
		{
			"cell_type": "code",
			"id": "imports_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": ["# gym 라이브러리가 없다면 설치합니다.\n", "!pip install gym"]
		},
		{
			"cell_type": "code",
			"id": "init_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": ["import gym\n", "import numpy as np\n", "import random\n", "import time\n", "from IPython.display import clear_output"]
		},
		{
			"cell_type": "markdown",
			"id": "env_q_learning",
			"metadata": {},
			"source": [
				"### (1) 환경(Environment) 생성\n",
				"FrozenLake-v1: 4x4 크기의 그리드 월드 환경.\n",
				"- `S`: 시작 지점\n",
				"- `F`: 얼어있는 길 (안전)\n",
				"- `H`: 구멍 (빠지면 -1 보상, 에피소드 종료)\n",
				"- `G`: 목표 지점 (도착하면 +1 보상, 에피소드 종료)"
			]
		},
		{
			"cell_type": "code",
			"id": "create_env_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"env = gym.make('FrozenLake-v1', is_slippery=True) # is_slippery=True: 미끄러운 환경\n",
				"state_space_size = env.observation_space.n\n",
				"action_space_size = env.action_space.n\n",
				"\n",
				"print(f\"State(상태) 공간 크기: {state_space_size}\")\n",
				"print(f\"Action(행동) 공간 크기: {action_space_size}\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "q_table_section_q_learning",
			"metadata": {},
			"source": [
				"### (2) Q-테이블 초기화 및 하이퍼파라미터 설정\n",
				"Q-테이블은 모든 상태-행동 쌍에 대한 가치(Q-value)를 저장하는 행렬입니다. 처음에는 모든 값을 0으로 초기화하고, 학습을 통해 점차 최적의 값으로 업데이트해 나갑니다."
			]
		},
		{
			"cell_type": "code",
			"id": "init_q_table_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"q_table = np.zeros((state_space_size, action_space_size))\n",
				"\n",
				"# 하이퍼파라미터\n",
				"total_episodes = 20000        # 총 학습 에피소드 수\n",
				"learning_rate = 0.1           # 학습률 (alpha)\n",
				"gamma = 0.99                  # 할인율 (미래 보상의 가치를 현재 가치로 환산하는 비율)\n",
				"\n",
				"epsilon = 1.0                 # 탐험(Exploration) 확률\n",
				"max_epsilon = 1.0             \n",
				"min_epsilon = 0.01            \n",
				"epsilon_decay_rate = 0.0001   # Epsilon 감소율 (더 많은 탐험을 위해 천천히 감소)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "train_section_q_learning",
			"metadata": {},
			"source": [
				"### (3) Q-러닝 알고리즘 학습\n",
				"에이전트는 각 타임스텝마다 Epsilon-Greedy 전략에 따라 행동을 선택하고, 환경으로부터 다음 상태(new_state)와 보상(reward)을 받습니다. 이 정보를 바탕으로 아래의 벨만 방정식을 사용하여 Q-테이블을 업데이트합니다.\n",
				"$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] $$"
			]
		},
		{
			"cell_type": "code",
			"id": "run_train_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"rewards = []\n",
				"\n",
				"for episode in range(total_episodes):\n",
				"    state = env.reset()\n",
				"    done = False\n",
				"    episode_rewards = 0\n",
				"    \n",
				"    while not done:\n",
				"        if random.uniform(0, 1) < epsilon:\n",
				"            action = env.action_space.sample() # 탐험: 무작위 행동 선택\n",
				"        else:\n",
				"            action = np.argmax(q_table[state, :]) # 활용: Q-값이 가장 높은 행동 선택\n",
				"            \n",
				"        new_state, reward, done, info = env.step(action)\n",
				"        \n",
				"        # Q-테이블 업데이트\n",
				"        old_value = q_table[state, action]\n",
				"        next_max = np.max(q_table[new_state, :])\n",
				"        new_value = old_value + learning_rate * (reward + gamma * next_max - old_value)\n",
				"        q_table[state, action] = new_value\n",
				"        \n",
				"        state = new_state\n",
				"        episode_rewards += reward\n",
				"        \n",
				"    # Epsilon 감소 (탐험 확률을 점차 줄여나감)\n",
				"    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate * episode)\n",
				"    rewards.append(episode_rewards)\n",
				"\n",
				"print(\"Training finished.\\n\")\n",
				"print(\"--- Learned Q-Table ---\")\n",
				"print(q_table)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eval_section_q_learning",
			"metadata": {},
			"source": ["### (4) 학습 결과 분석 및 시각화\n", "학습이 진행됨에 따라 에피소드당 평균 보상이 증가하는지 확인하여 학습이 잘 이루어졌는지 평가합니다."]
		},
		{
			"cell_type": "code",
			"id": "run_eval_q_learning",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# 보상 그래프 시각화\n",
				"rewards_per_thousand_episodes = np.split(np.array(rewards), total_episodes / 1000)\n",
				"count = 1000\n",
				"print(\"********Average reward per thousand episodes********\\n\")\n",
				"for r in rewards_per_thousand_episodes:\n",
				"    print(f\"{count}: {sum(r/1000)}\")\n",
				"    count += 1000\n",
				"\n",
				"# 학습된 정책으로 에이전트 실행\n",
				"for episode in range(3):\n",
				"    state = env.reset()\n",
				"    done = False\n",
				"    print(f\"\\n--- EPISODE {episode+1} ---\")\n",
				"    time.sleep(1)\n",
				"    \n",
				"    for step in range(100):\n",
				"        clear_output(wait=True)\n",
				"        env.render()\n",
				"        time.sleep(0.3)\n",
				"        \n",
				"        action = np.argmax(q_table[state, :])\n",
				"        new_state, reward, done, info = env.step(action)\n",
				"        \n",
				"        if done:\n",
				"            env.render()\n",
				"            if reward == 1:\n",
				"                print(\"****GOAL!****\")\n",
				"                time.sleep(2)\n",
				"            break\n",
				"        state = new_state\n",
				"        \n",
				"env.close()"
			]
		}
	],
	"metadata": {},
	"nbformat": 4,
	"nbformat_minor": 5
}
