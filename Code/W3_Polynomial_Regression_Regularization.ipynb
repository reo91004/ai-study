{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_poly_reg",
			"metadata": {},
			"source": [
				"# 다항 회귀 및 정규화 (Polynomial Regression & Regularization)\n",
				"\n",
				"**학습 목표:**\n",
				"- 단순 선형 회귀의 한계를 이해하고, 비선형 관계를 모델링하기 위해 **다항 회귀**를 사용합니다.\n",
				"- 모델의 복잡도가 증가할 때 발생하는 **과적합(Overfitting)** 문제를 시각적으로 확인합니다.\n",
				"- 과적합을 제어하기 위한 정규화(Regularization) 기법인 **Ridge(L2)** 및 **Lasso(L1)** 회귀를 적용하고 그 효과를 비교합니다.\n",
				"- `scikit-learn`의 `Pipeline`을 사용하여 데이터 전처리 및 모델 학습 과정을 효율적으로 구성하는 방법을 배웁니다."
			]
		},
		{
			"cell_type": "code",
			"id": "imports_poly_reg",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"import seaborn as sns\n",
				"\n",
				"from sklearn.model_selection import train_test_split\n",
				"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
				"from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
				"from sklearn.pipeline import make_pipeline\n",
				"from sklearn.metrics import mean_squared_error, r2_score"
			]
		},
		{
			"cell_type": "markdown",
			"id": "data_prep_poly_reg",
			"metadata": {},
			"source": ["### (1) 데이터 준비 및 탐색 (EDA)\n", "보스턴 주택 가격 데이터셋을 사용합니다. 이 중 'LSTAT'(하위 계층 비율) 특성과 'MEDV'(주택 가격) 간의 관계를 중심으로 분석하겠습니다."]
		},
		{
			"cell_type": "code",
			"id": "load_data_poly_reg",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
				"col_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
				"housing_df = pd.read_csv(url, delim_whitespace=True, names=col_names)\n",
				"\n",
				"# LSTAT과 MEDV만 사용\n",
				"X = housing_df[['LSTAT']]\n",
				"y = housing_df['MEDV']\n",
				"\n",
				"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
				"\n",
				"# LSTAT과 MEDV의 관계 시각화\n",
				"plt.figure(figsize=(8, 6))\n",
				"sns.scatterplot(x=X_train['LSTAT'], y=y_train)\n",
				"plt.title('LSTAT vs MEDV (Training Data)')\n",
				"plt.xlabel('LSTAT (Percentage of lower status of the population)')\n",
				"plt.ylabel('MEDV (Median value of owner-occupied homes in $1000s)')\n",
				"plt.show()\n",
				"print(\"데이터가 선형 관계보다는 곡선 형태의 비선형 관계를 보이는 것을 확인할 수 있습니다.\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "poly_reg_section",
			"metadata": {},
			"source": ["### (2) 다항 회귀를 이용한 비선형 관계 모델링\n", "단순 선형 회귀와 2차 다항 회귀 모델을 비교하여 비선형 모델의 성능 향상을 확인합니다."]
		},
		{
			"cell_type": "code",
			"id": "fit_poly_reg",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# 1차 선형 회귀 (단순 회귀)\n",
				"linear_model = LinearRegression()\n",
				"linear_model.fit(X_train, y_train)\n",
				"y_pred_linear = linear_model.predict(X_test)\n",
				"rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
				"\n",
				"# 2차 다항 회귀\n",
				"# make_pipeline으로 PolynomialFeatures와 LinearRegression을 연결\n",
				"poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
				"poly_model.fit(X_train, y_train)\n",
				"y_pred_poly = poly_model.predict(X_test)\n",
				"rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
				"\n",
				"print(f\"Linear Regression RMSE: {rmse_linear:.4f}\")\n",
				"print(f\"Polynomial Regression (Degree 2) RMSE: {rmse_poly:.4f}\")\n",
				"\n",
				"# 결과 시각화\n",
				"plt.figure(figsize=(10, 7))\n",
				"plt.scatter(X['LSTAT'], y, label='Actual Data', alpha=0.5)\n",
				"X_plot = np.sort(X['LSTAT'].unique()).reshape(-1, 1)\n",
				"plt.plot(X_plot, linear_model.predict(X_plot), color='red', linewidth=2, label=f'Linear Fit (RMSE: {rmse_linear:.2f})')\n",
				"plt.plot(X_plot, poly_model.predict(X_plot), color='green', linewidth=2, label=f'Polynomial Fit (RMSE: {rmse_poly:.2f})')\n",
				"plt.title('Linear vs Polynomial Regression')\n",
				"plt.xlabel('LSTAT')\n",
				"plt.ylabel('MEDV')\n",
				"plt.legend()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "overfitting_section",
			"metadata": {},
			"source": [
				"### (3) 과적합 문제 확인\n",
				"차수(degree)를 높여 모델을 더 복잡하게 만들면 훈련 데이터는 더 잘 예측하지만, 새로운 데이터(테스트 데이터)에 대한 성능은 오히려 나빠지는 과적합 현상이 발생합니다."
			]
		},
		{
			"cell_type": "code",
			"id": "show_overfitting",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# 10차 다항 회귀 모델\n",
				"overfit_model = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\n",
				"overfit_model.fit(X_train, y_train)\n",
				"y_pred_overfit = overfit_model.predict(X_test)\n",
				"rmse_overfit = np.sqrt(mean_squared_error(y_test, y_pred_overfit))\n",
				"\n",
				"# 시각화\n",
				"plt.figure(figsize=(10, 7))\n",
				"plt.scatter(X['LSTAT'], y, label='Actual Data', alpha=0.5)\n",
				"plt.plot(X_plot, poly_model.predict(X_plot), color='green', linewidth=2, label=f'Degree 2 (RMSE: {rmse_poly:.2f})')\n",
				"plt.plot(X_plot, overfit_model.predict(X_plot), color='purple', linewidth=2, label=f'Degree 10 (RMSE: {rmse_overfit:.2f})')\n",
				"plt.title('Overfitting with High-Degree Polynomial')\n",
				"plt.xlabel('LSTAT')\n",
				"plt.ylabel('MEDV')\n",
				"plt.ylim(0, 55)\n",
				"plt.legend()\n",
				"plt.show()\n",
				"print(\"10차 모델은 훈련 데이터의 노이즈까지 학습하여 예측선이 매우 불안정하며, 2차 모델보다 테스트 RMSE가 높습니다.\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "regularization_section",
			"metadata": {},
			"source": [
				"### (4) 정규화를 통한 과적합 제어\n",
				"복잡한 모델(10차 다항)에 Ridge(L2)와 Lasso(L1) 정규화를 적용하여 과적합을 완화합니다. 정규화 모델은 특성 스케일링에 영향을 받으므로 `StandardScaler`를 파이프라인에 추가합니다."
			]
		},
		{
			"cell_type": "code",
			"id": "apply_regularization",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"# Ridge 회귀 (alpha는 규제 강도)\n",
				"ridge_model = make_pipeline(PolynomialFeatures(degree=10), StandardScaler(), Ridge(alpha=1.0))\n",
				"ridge_model.fit(X_train, y_train)\n",
				"y_pred_ridge = ridge_model.predict(X_test)\n",
				"rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
				"\n",
				"# Lasso 회귀\n",
				"lasso_model = make_pipeline(PolynomialFeatures(degree=10), StandardScaler(), Lasso(alpha=0.1))\n",
				"lasso_model.fit(X_train, y_train)\n",
				"y_pred_lasso = lasso_model.predict(X_test)\n",
				"rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
				"\n",
				"print(f\"High-degree (10) Linear RMSE: {rmse_overfit:.4f}\")\n",
				"print(f\"High-degree (10) Ridge RMSE:   {rmse_ridge:.4f}\")\n",
				"print(f\"High-degree (10) Lasso RMSE:   {rmse_lasso:.4f}\")\n",
				"\n",
				"# 시각화\n",
				"plt.figure(figsize=(10, 7))\n",
				"plt.scatter(X['LSTAT'], y, label='Actual Data', alpha=0.5)\n",
				"plt.plot(X_plot, overfit_model.predict(X_plot), color='purple', linestyle='--', linewidth=2, label=f'Linear (Overfit) RMSE: {rmse_overfit:.2f}')\n",
				"plt.plot(X_plot, ridge_model.predict(X_plot), color='orange', linewidth=2, label=f'Ridge (Regularized) RMSE: {rmse_ridge:.2f}')\n",
				"plt.plot(X_plot, lasso_model.predict(X_plot), color='cyan', linewidth=2, label=f'Lasso (Regularized) RMSE: {rmse_lasso:.2f}')\n",
				"plt.title('Effect of Regularization on Overfitting')\n",
				"plt.xlabel('LSTAT')\n",
				"plt.ylabel('MEDV')\n",
				"plt.ylim(0, 55)\n",
				"plt.legend()\n",
				"plt.show()\n",
				"print(\"Ridge와 Lasso 모델이 과적합된 Linear 모델보다 훨씬 안정적인 예측선을 그리며, 테스트 RMSE도 낮은 것을 볼 수 있습니다.\")"
			]
		}
	],
	"metadata": {},
	"nbformat": 4,
	"nbformat_minor": 5
}
