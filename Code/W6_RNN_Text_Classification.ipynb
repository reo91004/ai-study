{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_rnn",
			"metadata": {},
			"source": [
				"# 순환 신경망(RNN)을 이용한 텍스트 분류\n",
				"\n",
				"**학습 목표:**\n",
				"- 순서가 중요한 시퀀스(sequence) 데이터 처리에 특화된 **순환 신경망(RNN)**의 원리를 이해합니다.\n",
				"- RNN의 장기 의존성(long-term dependency) 문제를 해결한 **LSTM(Long Short-Term Memory)** 모델의 구조를 학습합니다.\n",
				"- 텍스트 데이터를 신경망이 처리할 수 있도록 **단어 임베딩(Word Embedding)**과 **패딩(Padding)** 과정을 이해하고 적용합니다.\n",
				"- **TensorFlow/Keras**를 사용하여 LSTM 모델을 구축하고, **IMDB 영화 리뷰 데이터셋**으로 긍정/부정 감성 분석을 수행합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "imports_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"import tensorflow as tf\n",
				"from tensorflow import keras\n",
				"from tensorflow.keras.datasets import imdb\n",
				"from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
				"from tensorflow.keras.models import Sequential\n",
				"from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
				"import matplotlib.pyplot as plt"
			]
		},
		{
			"cell_type": "markdown",
			"id": "config_rnn",
			"metadata": {},
			"source": [
				"### (1) 하이퍼파라미터 설정"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "set_config_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"vocab_size = 10000  # 어휘 사전의 크기 (자주 등장하는 단어 10,000개)\n",
				"max_len = 256      # 각 리뷰의 최대 길이 (단어 수)\n",
				"embedding_dim = 128 # 단어를 표현할 임베딩 벡터의 차원\n",
				"batch_size = 64\n",
				"epochs = 5"
			]
		},
		{
			"cell_type": "markdown",
			"id": "data_prep_rnn",
			"metadata": {},
			"source": [
				"### (2) 데이터 준비: IMDB 영화 리뷰\n",
				"IMDB 데이터셋은 이미 전처리되어 각 단어가 고유한 정수 인덱스로 변환되어 있습니다. 신경망에 입력하기 위해서는 모든 시퀀스의 길이를 동일하게 맞춰야 하므로, `pad_sequences`를 사용하여 길이를 `max_len`으로 통일합니다. 길이가 짧은 리뷰는 0으로 채우고(padding), 긴 리뷰는 잘라냅니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "load_data_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
				"\n",
				"X_train_padded = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
				"X_test_padded = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
				"\n",
				"print(f\"Original sequence example (length {len(X_train[0])}):\\n{X_train[0]}\")\n",
				"print(f\"\\nPadded sequence example (length {len(X_train_padded[0])}):\\n{X_train_padded[0]}\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "model_def_rnn",
			"metadata": {},
			"source": [
				"### (3) 모델 정의: 양방향 LSTM (Bidirectional LSTM)\n",
				"- **Embedding Layer**: 정수 인덱스로 표현된 단어를 `embedding_dim` 차원의 밀집 벡터(dense vector)로 변환합니다. 이 과정에서 단어 간의 의미적 유사성을 학습합니다.\n",
				"- **Bidirectional(LSTM)**: 텍스트를 앞에서 뒤로, 그리고 뒤에서 앞으로 양방향으로 처리하여 문맥 정보를 더 풍부하게 학습합니다.\n",
				"- **Dropout**: 훈련 중 일부 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.\n",
				"- **Dense (sigmoid)**: 최종적으로 리뷰가 긍정(1)일 확률을 출력합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "create_model_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"model = Sequential([\n",
				"    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
				"    Bidirectional(LSTM(64, return_sequences=True)), # 다음 LSTM 층으로 시퀀스를 전달하기 위해 True\n",
				"    Bidirectional(LSTM(32)),\n",
				"    Dense(64, activation='relu'),\n",
				"    Dropout(0.5),\n",
				"    Dense(1, activation='sigmoid')\n",
				"])\n",
				"\n",
				"model.summary()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "compile_section_rnn",
			"metadata": {},
			"source": [
				"### (4) 모델 컴파일 및 훈련\n",
				"긍정/부정의 이진 분류 문제이므로 손실 함수로 `binary_crossentropy`를 사용합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "compile_model_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"model.compile(optimizer='adam',\n",
				"              loss='binary_crossentropy',\n",
				"              metrics=['accuracy'])\n",
				"\n",
				"history = model.fit(X_train_padded, y_train, \n",
				"                      batch_size=batch_size, \n",
				"                      epochs=epochs, \n",
				"                      validation_data=(X_test_padded, y_test))"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eval_section_rnn",
			"metadata": {},
			"source": [
				"### (5) 모델 평가 및 학습 과정 시각화"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "run_eval_rnn",
			"metadata": {},
			"outputs": [],
			"source": [
				"test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=2)\n",
				"print(f'\\nTest accuracy: {test_acc:.4f}')\n",
				"\n",
				"# 학습 과정 시각화\n",
				"history_df = pd.DataFrame(history.history)\n",
				"history_df.plot(figsize=(12, 5))\n",
				"plt.grid(True)\n",
				"plt.gca().set_ylim(0, 1)\n",
				"plt.title('Training and Validation Metrics')\n",
				"plt.show()"
			]
		}
	],
	"metadata": {
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
