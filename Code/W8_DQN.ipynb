{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_dqn",
			"metadata": {},
			"source": [
				"# 심층 Q-네트워크(DQN)로 CartPole 문제 풀기\n",
				"\n",
				"**학습 목표:**\n",
				"- 상태 공간이 매우 크거나 연속적인 문제에서 Q-테이블을 사용할 수 없는 한계를 이해하고, **딥러닝**을 결합한 DQN의 필요성을 학습합니다.\n",
				"- 상태(state)를 입력받아 각 행동(action)의 Q-값을 출력하는 **신경망(Q-Network)**을 설계하고 구현합니다.\n",
				"- 학습 안정성을 높이는 DQN의 핵심 기술인 **경험 리플레이(Experience Replay)**와 **타겟 네트워크(Target Network)**의 역할과 구현 방법을 배웁니다.\n",
				"- **OpenAI Gym**의 **CartPole** 환경에서 DQN 에이전트가 막대의 균형을 오랫동안 유지하도록 학습시킵니다."
			]
		},
		{
			"cell_type": "code",
			"id": "imports_dqn",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"import gym\n",
				"import numpy as np\n",
				"import random\n",
				"from collections import deque\n",
				"import tensorflow as tf\n",
				"from tensorflow.keras.models import Sequential\n",
				"from tensorflow.keras.layers import Dense\n",
				"from tensorflow.keras.optimizers import Adam\n",
				"import matplotlib.pyplot as plt"
			]
		},
		{
			"cell_type": "markdown",
			"id": "dqn_agent_class_dqn",
			"metadata": {},
			"source": [
				"### (1) DQN 에이전트 클래스 정의\n",
				"DQN 알고리즘에 필요한 모든 구성 요소(Q-네트워크, 타겟 네트워크, 메모리, 하이퍼파라미터 등)와 기능(행동 선택, 경험 저장, 학습)을 하나의 클래스로 캡슐화하여 코드를 구조적으로 관리합니다."
			]
		},
		{
			"cell_type": "code",
			"id": "define_dqn_agent_class_dqn",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"class DQNAgent:\n",
				"    def __init__(self, state_size, action_size):\n",
				"        self.state_size = state_size\n",
				"        self.action_size = action_size\n",
				"        \n",
				"        # 경험 리플레이 메모리 (deque)\n",
				"        self.memory = deque(maxlen=2000)\n",
				"        \n",
				"        # 하이퍼파라미터\n",
				"        self.gamma = 0.95    # discount factor\n",
				"        self.epsilon = 1.0   # exploration rate\n",
				"        self.epsilon_min = 0.01\n",
				"        self.epsilon_decay = 0.995\n",
				"        self.learning_rate = 0.001\n",
				"        \n",
				"        # Q-Network와 Target Network 생성\n",
				"        self.model = self._build_model()\n",
				"        self.target_model = self._build_model()\n",
				"        self.update_target_model()\n",
				"\n",
				"    def _build_model(self):\n",
				"        # Q-값을 근사할 신경망 모델\n",
				"        model = Sequential([\n",
				"            Dense(24, input_dim=self.state_size, activation='relu'),\n",
				"            Dense(24, activation='relu'),\n",
				"            Dense(self.action_size, activation='linear')\n",
				"        ])\n",
				"        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
				"        return model\n",
				"\n",
				"    def update_target_model(self):\n",
				"        # 타겟 네트워크의 가중치를 메인 네트워크의 가중치로 업데이트\n",
				"        self.target_model.set_weights(self.model.get_weights())\n",
				"\n",
				"    def remember(self, state, action, reward, next_state, done):\n",
				"        # 경험(s, a, r, s', done)을 메모리에 저장\n",
				"        self.memory.append((state, action, reward, next_state, done))\n",
				"\n",
				"    def act(self, state):\n",
				"        # Epsilon-greedy 정책으로 행동 선택\n",
				"        if np.random.rand() <= self.epsilon:\n",
				"            return random.randrange(self.action_size)\n",
				"        act_values = self.model.predict(state, verbose=0)\n",
				"        return np.argmax(act_values[0])\n",
				"\n",
				"    def replay(self, batch_size):\n",
				"        # 메모리에서 미니배치를 샘플링하여 학습 (경험 리플레이)\n",
				"        if len(self.memory) < batch_size:\n",
				"            return\n",
				"        minibatch = random.sample(self.memory, batch_size)\n",
				"        \n",
				"        states = np.array([t[0][0] for t in minibatch])\n",
				"        actions = np.array([t[1] for t in minibatch])\n",
				"        rewards = np.array([t[2] for t in minibatch])\n",
				"        next_states = np.array([t[3][0] for t in minibatch])\n",
				"        dones = np.array([t[4] for t in minibatch])\n",
				"\n",
				"        # 현재 상태에 대한 Q-값과 다음 상태에 대한 타겟 Q-값 예측\n",
				"        q_values = self.model.predict(states, verbose=0)\n",
				"        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
				"\n",
				"        # 벨만 방정식을 이용한 타겟값 계산\n",
				"        targets = rewards + self.gamma * np.amax(target_q_values, axis=1) * (1 - dones)\n",
				"        \n",
				"        # 실제 선택했던 행동에 대한 Q-값만 타겟값으로 업데이트\n",
				"        q_values[np.arange(batch_size), actions] = targets\n",
				"\n",
				"        # 모델 학습\n",
				"        self.model.fit(states, q_values, epochs=1, verbose=0)\n",
				"        \n",
				"        # Epsilon 감소\n",
				"        if self.epsilon > self.epsilon_min:\n",
				"            self.epsilon *= self.epsilon_decay"
			]
		},
		{
			"cell_type": "markdown",
			"id": "train_section_dqn",
			"metadata": {},
			"source": [
				"### (2) DQN 학습 실행\n",
				"CartPole-v1 환경에서 에이전트를 생성하고, 지정된 에피소드 수만큼 학습을 진행합니다. 각 에피소드는 막대가 쓰러지거나 최대 타임스텝(500)에 도달하면 종료됩니다."
			]
		},
		{
			"cell_type": "code",
			"id": "run_train_dqn",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"env = gym.make('CartPole-v1')\n",
				"state_size = env.observation_space.shape[0]\n",
				"action_size = env.action_space.n\n",
				"agent = DQNAgent(state_size, action_size)\n",
				"\n",
				"episodes = 500\n",
				"batch_size = 64\n",
				"update_target_every = 10\n",
				"scores = []\n",
				"\n",
				"for e in range(episodes):\n",
				"    state = env.reset()\n",
				"    state = np.reshape(state, [1, state_size])\n",
				"    done = False\n",
				"    time_steps = 0\n",
				"    \n",
				"    while not done:\n",
				"        time_steps += 1\n",
				"        action = agent.act(state)\n",
				"        next_state, reward, done, _ = env.step(action)\n",
				"        reward = reward if not done or time_steps == 500 else -10 # 실패 시 음수 보상\n",
				"        next_state = np.reshape(next_state, [1, state_size])\n",
				"        \n",
				"        agent.remember(state, action, reward, next_state, done)\n",
				"        state = next_state\n",
				"        \n",
				"        if done:\n",
				"            scores.append(time_steps)\n",
				"            print(f\"episode: {e+1}/{episodes}, score: {time_steps}, e: {agent.epsilon:.2}\")\n",
				"            break\n",
				"    \n",
				"    agent.replay(batch_size)\n",
				"    \n",
				"    if e % update_target_every == 0:\n",
				"        agent.update_target_model()\n",
				"        \n",
				"env.close()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eval_section_dqn",
			"metadata": {},
			"source": ["### (3) 학습 결과 시각화"]
		},
		{
			"cell_type": "code",
			"id": "run_eval_dqn",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"source": [
				"plt.figure(figsize=(12, 6))\n",
				"plt.plot(scores)\n",
				"plt.title('DQN Agent Training on CartPole-v1')\n",
				"plt.xlabel('Episode')\n",
				"plt.ylabel('Score (Time Steps)')\n",
				"plt.grid(True)\n",
				"plt.show()\n",
				"print(\"학습이 진행됨에 따라 에이전트가 더 오랫동안 균형을 유지하는 것을 볼 수 있습니다.\")"
			]
		}
	],
	"metadata": {},
	"nbformat": 4,
	"nbformat_minor": 5
}
