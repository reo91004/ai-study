{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules and Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package: \n",
    "### mlxtend\n",
    "- mlxtend는 “Machine Learning Extensions”의 약자로, Scikit-learn과 함께 사용하기 위해 설계된 Python 라이브러리입니다.\n",
    "- mlxtend는 Scikit-learn에 없는 여러 추가적인 기능을 제공하며, 특히 데이터 전처리, 모델 평가, 시각화, 그리고 연관 규칙 마이닝 같은 알고리즘 구현에 강점을 가지고 있습니다.\n",
    "\n",
    "### surprise\n",
    "- surprise는 추천 시스템 구축을 위해 설계된 Python 라이브러리입니다.\n",
    "- 공식 명칭은 “Simple Python Recommendation System”이며, 사용자-아이템 기반의 추천 알고리즘을 쉽게 구현할 수 있도록 설계되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend==0.22.0 in /opt/anaconda3/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (1.11.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (3.8.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from mlxtend==0.22.0) (68.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend==0.22.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->mlxtend==0.22.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->mlxtend==0.22.0) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.2->mlxtend==0.22.0) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend==0.22.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend==0.22.0 \n",
    "!pip install surprise\n",
    "!pip install dmba # for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "import dmba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \bAssociation Rule Mining\n",
    "- 휴대폰케이스거래데이터(faceplate)를 기반으로 연관규칙 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data set \n",
    "fp_df = dmba.load_data('Faceplate.csv')\n",
    "fp_df.set_index('Transaction', inplace=True)\n",
    "fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequent itemsets\n",
    "itemsets = apriori(fp_df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# and convert into rules\n",
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.5) \n",
    "rules.sort_values(by=['lift'], ascending=False).head(6)\n",
    "\n",
    "print(rules.sort_values(by=['lift'], ascending=False)\n",
    "      .drop(columns=['antecedent support', 'consequent support', 'conviction','zhangs_metric'])\n",
    "      .head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to get rules with single consequents only\n",
    "rules[[len(c) == 1 for c in rules.consequents]].sort_values(by=['lift'], ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apriori method accepts sparse data frames as well. If we convert the original data frame to sparse format, we can see that the memory requirements go down to 40%. The `fill_value` argument informs the `to_sparse` method here which fields to ignore in each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data set into a sparse data frame\n",
    "sparse_df = fp_df.astype(pd.SparseDtype(int, fill_value=0))\n",
    "print('Density {}'.format(sparse_df.sparse.density))\n",
    "\n",
    "# create frequent itemsets\n",
    "itemsets = apriori(sparse_df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# and convert into rules\n",
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\n",
    "rules.sort_values(by=['lift'], ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "nratings = 5000\n",
    "randomData = pd.DataFrame({\n",
    "    'itemID': [random.randint(0,99) for _ in range(nratings)],\n",
    "    'userID': [random.randint(0,999) for _ in range(nratings)],\n",
    "    'rating': [random.randint(1,5) for _ in range(nratings)],\n",
    "})\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    byUser = defaultdict(list)\n",
    "    for p in predictions:\n",
    "        byUser[p.uid].append(p)\n",
    "    \n",
    "    # For each user, reduce predictions to top-n\n",
    "    for uid, userPredictions in byUser.items():\n",
    "        byUser[uid] = heapq.nlargest(n, userPredictions, key=lambda p: p.est)\n",
    "    return byUser\n",
    "\n",
    "# Convert thes data set into the format required by the surprise package\n",
    "# The columns must correspond to user id, item id and ratings (in that order)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(randomData[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=1)\n",
    "\n",
    "## User-based filtering\n",
    "# compute cosine similarity between users \n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = get_top_n(predictions, n=4)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "print()\n",
    "print('Top-4 recommended items for each user')\n",
    "for uid, user_ratings in list(top_n.items())[:5]:\n",
    "    print('User {}'.format(uid))\n",
    "    for prediction in user_ratings:\n",
    "        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Item-based filtering\n",
    "# compute cosine similarity between users \n",
    "sim_options = {'name': 'cosine', 'user_based': False}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "predictions = algo.test(testset)\n",
    "top_n = get_top_n(predictions, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the recommended items for each user\n",
    "print()\n",
    "print('Top-4 recommended items for each user')\n",
    "for uid, user_ratings in list(top_n.items())[:5]:\n",
    "    print('User {}'.format(uid))\n",
    "    for prediction in user_ratings:\n",
    "        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a model using the full dataset\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'cosine', 'user_based': False}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict rating for user 383 and item 7\n",
    "algo.predict(383, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
