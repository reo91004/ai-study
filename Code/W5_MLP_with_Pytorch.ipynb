{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "intro_pytorch",
			"metadata": {},
			"source": [
				"# PyTorch로 구현하는 다층 퍼셉트론 (MLP)\n",
				"\n",
				"**학습 목표:**\n",
				"- 대표적인 딥러닝 프레임워크인 **PyTorch**의 기본 구조와 사용법을 익힙니다.\n",
				"- 딥러닝의 'Hello, World!'와 같은 **MNIST 손글씨 숫자** 데이터셋을 처리하는 방법을 배웁니다. (`Dataset`, `DataLoader`)\n",
				"- `nn.Module`을 상속받아 **다층 퍼셉트론(MLP)** 모델을 직접 정의하고, 각 계층의 역할을 이해합니다.\n",
				"- 손실 함수(Cross-Entropy), 옵티마이저(Adam)를 설정하고, 모델을 훈련시키는 전체 과정을 단계별로 학습합니다.\n",
				"- 훈련 과정의 손실과 정확도를 시각화하여 모델의 학습 상태를 모니터링합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "imports_pytorch",
			"metadata": {},
			"outputs": [],
			"source": [
				"import torch\n",
				"import torch.nn as nn\n",
				"import torch.optim as optim\n",
				"from torchvision import datasets, transforms\n",
				"from torch.utils.data import DataLoader\n",
				"import matplotlib.pyplot as plt\n",
				"import numpy as np"
			]
		},
		{
			"cell_type": "markdown",
			"id": "config_pytorch",
			"metadata": {},
			"source": [
				"### (1) 하이퍼파라미터 및 장치 설정\n",
				"모델 훈련에 필요한 주요 변수들을 설정합니다. GPU 사용이 가능할 경우, 연산 장치를 'cuda'로 설정하여 학습 속도를 높입니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "set_config_pytorch",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using device: cpu\n"
					]
				}
			],
			"source": [
				"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
				"print(f'Using device: {device}')\n",
				"\n",
				"# 하이퍼파라미터\n",
				"input_size = 28 * 28  # MNIST 이미지 크기\n",
				"hidden_size1 = 512\n",
				"hidden_size2 = 256\n",
				"num_classes = 10\n",
				"learning_rate = 0.001\n",
				"batch_size = 64\n",
				"epochs = 10"
			]
		},
		{
			"cell_type": "markdown",
			"id": "data_prep_pytorch",
			"metadata": {},
			"source": [
				"### (2) 데이터 준비: MNIST\n",
				"- `torchvision.transforms`를 사용하여 데이터를 텐서(Tensor)로 변환하고, 픽셀 값을 정규화하는 전처리 파이프라인을 구성합니다.\n",
				"- `DataLoader`는 데이터를 미니배치(mini-batch) 단위로 묶고, 훈련 시 데이터를 섞어주는(shuffle) 역할을 수행하여 효율적인 학습을 돕습니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "load_data_pytorch",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
						"Failed to download (trying next):\n",
						"HTTP Error 404: Not Found\n",
						"\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"100%|██████████| 9912422/9912422 [00:04<00:00, 2045310.18it/s]\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
						"\n",
						"Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
						"Failed to download (trying next):\n",
						"HTTP Error 404: Not Found\n",
						"\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"100%|██████████| 28881/28881 [00:00<00:00, 139047.52it/s]\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
						"\n",
						"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
						"Failed to download (trying next):\n",
						"HTTP Error 404: Not Found\n",
						"\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"100%|██████████| 1648877/1648877 [00:01<00:00, 1439144.60it/s]\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
						"\n",
						"Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
						"Failed to download (trying next):\n",
						"HTTP Error 404: Not Found\n",
						"\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
						"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"100%|██████████| 4542/4542 [00:00<00:00, 3839284.31it/s]"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
						"\n",
						"Images batch shape: torch.Size([64, 1, 28, 28])\n",
						"Labels batch shape: torch.Size([64])\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"transform = transforms.Compose([\n",
				"    transforms.ToTensor(), # 이미지를 PyTorch 텐서로 변환\n",
				"    transforms.Normalize((0.1307,), (0.3081,)) # MNIST 데이터의 평균과 표준편차로 정규화\n",
				"])\n",
				"\n",
				"train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
				"test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
				"\n",
				"train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
				"test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
				"\n",
				"# 데이터 확인\n",
				"images, labels = next(iter(train_loader))\n",
				"print(f\"Images batch shape: {images.shape}\")\n",
				"print(f\"Labels batch shape: {labels.shape}\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "model_def_pytorch",
			"metadata": {},
			"source": [
				"### (3) 모델 정의: MLP\n",
				"`nn.Module`을 상속받아 사용자 정의 모델 클래스를 만듭니다. `__init__` 메서드에서 필요한 계층(layer)들을 정의하고, `forward` 메서드에서 데이터가 이 계층들을 통과하는 순서(순전파)를 정의합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "create_model_pytorch",
			"metadata": {},
			"outputs": [],
			"source": [
				"class MLP(nn.Module):\n",
				"    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
				"        super(MLP, self).__init__()\n",
				"        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
				"        self.relu1 = nn.ReLU()\n",
				"        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
				"        self.relu2 = nn.ReLU()\n",
				"        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
				"\n",
				"    def forward(self, x):\n",
				"        # 입력 데이터를 1차원으로 평탄화\n",
				"        x = x.view(-1, 28*28)\n",
				"        out = self.fc1(x)\n",
				"        out = self.relu1(out)\n",
				"        out = self.fc2(out)\n",
				"        out = self.relu2(out)\n",
				"        out = self.fc3(out)\n",
				"        return out\n",
				"\n",
				"model = MLP(input_size, hidden_size1, hidden_size2, num_classes).to(device)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "loss_optim_pytorch",
			"metadata": {},
			"source": [
				"### (4) 손실 함수 및 옵티마이저 설정\n",
				"- **손실 함수 (Loss Function)**: 다중 클래스 분류 문제이므로 `nn.CrossEntropyLoss`를 사용합니다. 이 함수는 내부적으로 Softmax 함수를 적용한 후 Cross-Entropy를 계산하므로 모델의 마지막 계층에 활성화 함수를 추가할 필요가 없습니다.\n",
				"- **옵티마이저 (Optimizer)**: 경사 하강법을 통해 모델의 가중치를 업데이트하는 역할을 합니다. 가장 널리 사용되는 `Adam`을 사용합니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"id": "set_loss_optim_pytorch",
			"metadata": {},
			"outputs": [],
			"source": [
				"criterion = nn.CrossEntropyLoss()\n",
				"optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "train_section_pytorch",
			"metadata": {},
			"source": [
				"### (5) 모델 훈련\n",
				"전체 훈련 데이터를 여러 번 반복하여 학습합니다(에포크). 각 에포크마다 `DataLoader`에서 미니배치를 가져와 다음 과정을 반복합니다.\n",
				"1.  **순전파(Forward pass)**: 입력 데이터를 모델에 통과시켜 예측값을 얻습니다.\n",
				"2.  **손실 계산**: 예측값과 실제 정답을 비교하여 손실을 계산합니다.\n",
				"3.  **역전파(Backward pass)**: 손실을 최소화하기 위해 각 가중치에 대한 그래디언트(기울기)를 계산합니다 (`loss.backward()`).\n",
				"4.  **가중치 업데이트**: 옵티마이저가 계산된 그래디언트를 사용하여 모델의 가중치를 업데이트합니다 (`optimizer.step()`)."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"id": "run_train_pytorch",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Epoch [1/10], Loss: 0.0210\n",
						"Epoch [2/10], Loss: 0.0088\n",
						"Epoch [3/10], Loss: 0.1561\n",
						"Epoch [4/10], Loss: 0.1720\n",
						"Epoch [5/10], Loss: 0.0227\n",
						"Epoch [6/10], Loss: 0.0081\n",
						"Epoch [7/10], Loss: 0.0026\n",
						"Epoch [8/10], Loss: 0.0175\n",
						"Epoch [9/10], Loss: 0.1459\n",
						"Epoch [10/10], Loss: 0.0009\n"
					]
				}
			],
			"source": [
				"for epoch in range(epochs):\n",
				"    model.train() # 모델을 훈련 모드로 설정\n",
				"    for batch_idx, (data, targets) in enumerate(train_loader):\n",
				"        # 데이터를 설정한 장치로 이동\n",
				"        data = data.to(device)\n",
				"        targets = targets.to(device)\n",
				"        \n",
				"        # 순전파\n",
				"        scores = model(data)\n",
				"        loss = criterion(scores, targets)\n",
				"        \n",
				"        # 역전파 및 가중치 업데이트\n",
				"        optimizer.zero_grad() # 그래디언트 초기화\n",
				"        loss.backward()       # 그래디언트 계산\n",
				"        optimizer.step()      # 가중치 업데이트\n",
				"        \n",
				"    # 에포크마다 결과 출력\n",
				"    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eval_section_pytorch",
			"metadata": {},
			"source": [
				"### (6) 모델 평가\n",
				"학습이 완료된 모델의 성능을 테스트 데이터셋으로 평가합니다. 평가 시에는 불필요한 그래디언트 계산을 비활성화(`torch.no_grad()`)하여 메모리 사용량을 줄이고 계산 속도를 높입니다."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"id": "run_eval_pytorch",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Accuracy on training set: 99.45%\n",
						"Accuracy on test set: 97.98%\n"
					]
				}
			],
			"source": [
				"def check_accuracy(loader, model):\n",
				"    model.eval() # 모델을 평가 모드로 설정\n",
				"    num_correct = 0\n",
				"    num_samples = 0\n",
				"    with torch.no_grad():\n",
				"        for x, y in loader:\n",
				"            x = x.to(device)\n",
				"            y = y.to(device)\n",
				"            \n",
				"            scores = model(x)\n",
				"            _, predictions = scores.max(1)\n",
				"            num_correct += (predictions == y).sum()\n",
				"            num_samples += predictions.size(0)\n",
				"            \n",
				"    accuracy = float(num_correct) / float(num_samples) * 100\n",
				"    return accuracy\n",
				"\n",
				"train_accuracy = check_accuracy(train_loader, model)\n",
				"test_accuracy = check_accuracy(test_loader, model)\n",
				"\n",
				"print(f'Accuracy on training set: {train_accuracy:.2f}%')\n",
				"print(f'Accuracy on test set: {test_accuracy:.2f}%')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a0225d56",
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "torch-mac",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.2"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
